# -*- coding: utf-8 -*-
"""TransferLearning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lDQ8R8xmjG1AuPhBmORhSMsNIU8H08SY

## First Imports
"""

# example of tending the vgg16 model
from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten
from __future__ import absolute_import, division, print_function, unicode_literals
import matplotlib.pylab as plt
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import pandas as pd
import os

# Importing all necessary libraries 
from keras.preprocessing.image import ImageDataGenerator 
from keras.models import Sequential 
from keras.layers import Conv2D, MaxPooling2D 
from keras.layers import Activation, Dropout, Flatten, Dense 
from keras import backend as K 
  

from google.colab import drive
drive.mount('/content/drive')

"""## Make the generators"""

# make sure to match original model's preprocessing function 
from keras.applications.resnet50 import preprocess_input  
from keras.preprocessing.image import ImageDataGenerator 
  
train_gen = ImageDataGenerator( 
        validation_split = 0.2,  
        preprocessing_function = preprocess_input) 
train_flow = train_gen.flow_from_directory("/content/drive/My Drive/Colab Notebooks/dataSet/TrainingSet/",  
                                           target_size =(256, 256),  
                                           batch_size = 32,  
                                           subset ="training") 
  
valid_flow = train_gen.flow_from_directory("/content/drive/My Drive/Colab Notebooks/dataSet/TrainingSet/",  
                                           target_size =(256, 256),  
                                           batch_size = 32,  
                                           subset ="validation") 
  
test_gen = ImageDataGenerator( 
        preprocessing_function = preprocess_input) 
test_flow = test_gen.flow_from_directory("/content/drive/My Drive/Colab Notebooks/dataSet/TestSet/",  
                                         target_size =(256, 256),  
                                         batch_size = 32)

"""## Load the pretrained model"""

from keras.applications.resnet50 import ResNet50 
from keras.layers import GlobalAveragePooling2D, Dense 
from keras.layers import BatchNormalization, Dropout 
from keras.models import Model 
  
# by default, the loaded model will include the original CNN  
#classifier designed for the ImageNet dataset 
# since we want to reuse this model for a different problem, 
# we need to omit the original fully connected layers, and  
# replace them with our own setting include_top = False will 
# load the model without the fully connected layer 
  
# load resnet model, with pretrained imagenet weights. 
res = ResNet50(weights ='imagenet', include_top = False,  
               input_shape =(256, 256, 3))

"""## Composite model layers and compile model"""

# get the output from the loaded model 
x = res.output  
  
# avg. pools across the spatial dimensions (rows, columns)  
# until it becomes zero. Reshapes data into a 1D, allowing  
# for proper input shape into Dense layers  
# (e.g. (8, 8, 2048) -> (2048)). 
x = GlobalAveragePooling2D()(x)  
  
# subtracts batch mean and divides by batch standard deviation 
# to reduce shift in input distributions between layers.  
x = BatchNormalization()(x)  
  
# dropout allows layers to be less dependent on 
# certain features, reducing overfitting 
x = Dropout(0.5)(x)  
x = Dense(512, activation ='relu')(x) 
x = BatchNormalization()(x) 
x = Dropout(0.5)(x) 
  
# output classification layer, we have 101 classes,  
# so we need 101 output neurons 
x = Dense(5, activation ='softmax')(x)  
  
# create the model, setting input / output 
model = Model(res.input, x)  
  
# compile the model - we're training using the Adam Optimizer 
# and Categorical Cross Entropy as the loss function 
model.compile(optimizer ='Adam',  
              loss ='categorical_crossentropy',  
              metrics =['accuracy'])  
  
# structure of our model 
model.summary()

"""## Fit the model"""

history = model.fit_generator(train_flow, epochs = 5, validation_data = valid_flow)

"""## Evaluate the model with the Testset"""

result = model.evaluate(test_flow) 
  
print('The model achieved a loss of %.2f and,'
      'accuracy of %.2f%%.' % (result[0], result[1]*100))

"""## Get the keys of the training"""

print(history.history.keys())

"""## Plot the learning Curves"""

acc = history.history['acc']
val_acc = history.history['val_acc']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0,1.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()